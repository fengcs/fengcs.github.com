<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[八、提升方法--统计机器学习方法]]></title>
    <url>%2F2017%2F10%2F30%2Ftj-boosting%2F</url>
    <content type="text"><![CDATA[AdaBoost算法 AdaBoost算法 输入：训练数据集 T={(x1,y1),(x2,y2),...,(xN,yN)}T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \}T={(x​1​​,y​1​​),(x​2​​,y​2​​),...,(x​N​​,y​N​​)} ，其中 xi∈Rnx_i\in R^nx​i​​∈R​n​​ 为实例的特征向量 ， yi∈Y={−1,+1}y_i\in Y= \{-1,+1\}y​i​​∈Y={−1,+1} 为实例的类别，弱学习算法 输出：最终分类器G(x) 初始化训练数据的权值分布 D1=(w11,...,w1i,...,w1N),w1i=1N,i=1,2,...,ND_1=(w_{11}, ...,w_{1i},...,w_{1N}), w_{1i}=\frac{1}{N}, i=1,2,...,N D​1​​=(w​11​​,...,w​1i​​,...,w​1N​​),w​1i​​=​N​​1​​,i=1,2,...,N 对m=1,2,…,M 使用具有权值分DmD_mD​m​​的训练数据集学习，得到基本分类器 Gm(x):χ→{−1,+1}G_m(x):\chi\rightarrow \{-1, +1\} G​m​​(x):χ→{−1,+1} 计算Gm(x)G_m(x)G​m​​(x)在训练数据集上的分类误差率 em(x)=P(Gm(xi)≠yi)=∑i=1NwmiI(Gm(xi)≠yi)e_m(x)=P(G_m(x_i) \neq y_i) = \sum_{i=1}^N w_{mi}I(G_m(x_i) \neq y_i) e​m​​(x)=P(G​m​​(x​i​​)≠y​i​​)=​i=1​∑​N​​w​mi​​I(G​m​​(x​i​​)≠y​i​​) 计算Gm(x)G_m(x)G​m​​(x)的系数 αm=12ln1−emem\alpha_m = \frac{1}{2} ln \frac{1-e_m}{e_m} α​m​​=​2​​1​​ln​e​m​​​​1−e​m​​​​ 更新训练数据集的权值分布 wm+1,i=wmiZmexp(−αmyiGm(xi))w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)) w​m+1,i​​=​Z​m​​​​w​mi​​​​exp(−α​m​​y​i​​G​m​​(x​i​​)) 构建基本分类器的组合 f(x)=∑m=1MαmGm(x)f(x)= \sum_{m=1}^M \alpha_m G_m(x) f(x)=​m=1​∑​M​​α​m​​G​m​​(x) 得到最终分类器 G(x)=sign(f(x))=sign(∑m=1MαmGm(x))G(x) = sign(f(x)) = sign\left( \sum_{m=1}^M \alpha_m G_m(x) \right) G(x)=sign(f(x))=sign(​m=1​∑​M​​α​m​​G​m​​(x)) 说明： 当 em≤1/2e_m\leq1/2e​m​​≤1/2时， αm\alpha_mα​m​​随着eme_me​m​​的减小而增大，所以分类误差率越小的分类器在最终分类器中作用越大 被基本分类器误分类的样本权值得到扩大 AdaBoost算法解释 AdaBoost模型是模型为加法模型，损失函数为指数函数，学习算法为前项分步算法的二分类学习方法。 前向分步算法： 提升树（Boosting Tree） 以决策树为基函数的提升方法称为提升树，对分类问题使用二叉分类树，对回归问题使用二叉回归树。]]></content>
      <tags>
        <tag>machine-learning</tag>
        <tag>boosting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[七、支持向量机--统计机器学习方法]]></title>
    <url>%2F2017%2F10%2F30%2Ftj-support-vector%2F</url>
    <content type="text"><![CDATA[线性可分支持向量机与硬间隔最大化 线性可分支持向量机 给定线性可分数据集，通过间隔最大化得到分离超平面为 w~⋅x+b~=0\tilde{w} \cdot x + \tilde{b} = 0 ​w​~​​⋅x+​b​~​​=0 以及相应的决策函数： f(x)=sign(w~⋅x+b~)f(x) = sign( \tilde{w} \cdot x + \tilde{b} ) f(x)=sign(​w​~​​⋅x+​b​~​​) 称为线性可分支持向量机 间隔函数与几何间隔 对于给定的训练数据集T和超平面(w,b)，定义超平面关于样本点的函数间隔为： γi^=yi(w⋅xi+b)\hat{\gamma_i} = y_i( w \cdot x_i + b ) ​γ​i​​​^​​=y​i​​(w⋅x​i​​+b) 定义超平面(w,b)关于训练数据集T的函数间隔为： γ^=mini=1,2,...,Nγi^\hat{\gamma} = \min_{i=1,2,...,N} \hat{\gamma_i} ​γ​^​​=​i=1,2,...,N​min​​​γ​i​​​^​​ 几何间隔，添加如下约束:∥w∥=1\begin{Vmatrix} w \end{Vmatrix}=1​∥​∥​​​w​​​∥​∥​​=1 间隔最大化 凸优化问题： minw=f(w)\min_w = f(w) ​w​min​​=f(w) stgi(w)≤0,i=1,2,...,kst\quad g_i(w) \le 0, i=1,2,...,k stg​i​​(w)≤0,i=1,2,...,k hi(w)=0,i=1,2,...,l\ \quad h_i(w) = 0, i=1,2,...,l h​i​​(w)=0,i=1,2,...,l 其中f(w) 与g(w)是连续可微的凸函数，h(w)为仿射函数。 几何间隔最大化可表述为如下约束最优化问题： maxw,bγ\max_{w,b} \gamma ​w,b​max​​γ styi(w∥w∥⋅xi+b∥w∥)≥γ,i=1,2,...,N st\quad y_i \left(\frac{w}{\begin{Vmatrix} w \end{Vmatrix}} \cdot x_i+\frac{b}{\begin{Vmatrix} w \end{Vmatrix}} \right) \ge \gamma, i=1,2,...,N sty​i​​(​​∥​∥​​​w​​​∥​∥​​​​w​​⋅x​i​​+​​∥​∥​​​w​​​∥​∥​​​​b​​)≥γ,i=1,2,...,N 令 γ=γ^∥w∥\gamma =\frac { \hat \gamma}{\begin{Vmatrix} w \end{Vmatrix}}{} γ=​​∥​∥​​​w​​​∥​∥​​​​​γ​^​​​​ 再令 γ^=1\hat \gamma=1​γ​^​​=1 可等价为如下形式： minw,b12∥w∥2\min_{w,b} \frac{1}{2} \begin{Vmatrix} w \end{Vmatrix}^2 ​w,b​min​​​2​​1​​​∥​∥​​​w​​​∥​∥​​​2​​ styi(w⋅xi+b)−1≥0,i=1,2,...,Nst\quad y_i \left(w \cdot x_i+b \right) -1 \ge 0, i=1,2,...,N sty​i​​(w⋅x​i​​+b)−1≥0,i=1,2,...,N 正常求法：todo 对偶方式求法：todo 线性支持向量机与软间隔最大化 对于线性不可分情况，添加松弛变量，线性不可分的线性支持向量机的学习问题变为如下凸二次规划： minw,b,ξ12∥w∥2+C∑i=1Nξi\min_{w,b,\xi} \frac{1}{2} \begin{Vmatrix} w \end{Vmatrix}^2 + C\sum_{i=1}^N \xi_i ​w,b,ξ​min​​​2​​1​​​∥​∥​​​w​​​∥​∥​​​2​​+C​i=1​∑​N​​ξ​i​​ styi(w⋅xi+b)≥1−ξi,i=1,2,...,Nst\quad y_i \left(w \cdot x_i+b \right) \ge 1- \xi_i , i=1,2,...,N sty​i​​(w⋅x​i​​+b)≥1−ξ​i​​,i=1,2,...,N ξi≥0,i=1,2,...,N\xi_i \ge 0 , i=1,2,...,N ξ​i​​≥0,i=1,2,...,N 通过求解上述凸二次规划问题，得到的超平面 w˙⋅x+b˙=0\dot w \cdot x+ \dot b = 0 ​w​˙​​⋅x+​b​˙​​=0 及相应的分类决策函数 f(x)=sign(w˙⋅x+b˙)f(x) = sign(\dot w \cdot x+ \dot b) f(x)=sign(​w​˙​​⋅x+​b​˙​​) 称为线性支持向量机。 非线性支持向量机与核函数 todo]]></content>
      <tags>
        <tag>machine-learning</tag>
        <tag>support vector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[六、逻辑斯谛回归与最大熵模型]]></title>
    <url>%2F2017%2F10%2F30%2Ftj-logistic%2F</url>
    <content type="text"><![CDATA[logistic回归模型 logistic分布 X具有如下的分布函数与概率密度函数，则称X服从逻辑斯蒂分布。 F(x)=P(X≤x)=11+e−(x−μ)/γF(x)=P(X\leq x)=\frac{1}{1+e^{-(x-\mu)/\gamma}} F(x)=P(X≤x)=​1+e​−(x−μ)/γ​​​​1​​ f(x)=F′(x)=e−(x−μ)/γγ(1+e−(x−μ)/γ)2f(x)=F&#x27;(x)=\frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2} f(x)=F​′​​(x)=​γ(1+e​−(x−μ)/γ​​)​2​​​​e​−(x−μ)/γ​​​​ 二项逻辑斯蒂回归模型 如下条件概率分布： P(Y=1∣x)=exp(w⋅x+b)1+exp(w⋅x+b)P(Y=1|x)=\frac{exp(w \cdot x+b)}{1+exp(w \cdot x+b)} P(Y=1∣x)=​1+exp(w⋅x+b)​​exp(w⋅x+b)​​ P(Y=0∣x)=11+exp(w⋅x+b)P(Y=0|x)=\frac{1}{1+exp(w \cdot x+b)} P(Y=0∣x)=​1+exp(w⋅x+b)​​1​​ 多项逻辑斯蒂回归模型 概率分布如下： P(Y=k∣x)=exp(wk⋅x)1+∑k=1K−1exp(wk⋅x)P(Y=k|x)=\frac{exp(w_k \cdot x)}{1+\sum_{k=1}^{K-1}exp(w_k \cdot x)} P(Y=k∣x)=​1+∑​k=1​K−1​​exp(w​k​​⋅x)​​exp(w​k​​⋅x)​​ P(Y=K∣x)=11+∑k=1K−1exp(wk⋅x)P(Y=K|x)=\frac{1}{1+\sum_{k=1}^{K-1}exp(w_k \cdot x)} P(Y=K∣x)=​1+∑​k=1​K−1​​exp(w​k​​⋅x)​​1​​ 最大熵模型 最大熵原理：熵最大的模型是最好的模型 离散随机变量X的概率分布是P(x)，其熵定义如下 H(P)=−∑xP(x)logP(x)H(P) = -\sum_x P(x)logP(x) H(P)=−​x​∑​​P(x)logP(x) 定义条件概率P(Y|X)上的条件熵为： H(P)=−∑x,yP~(x)P(y∣x)logP(y∣x)H(P) = -\sum_{x,y} \tilde{P}(x)P(y|x)logP(y|x) H(P)=−​x,y​∑​​​P​~​​(x)P(y∣x)logP(y∣x)]]></content>
      <tags>
        <tag>machine-learning</tag>
        <tag>logistic-regression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[五、决策树--统计机器学习方法]]></title>
    <url>%2F2017%2F10%2F30%2Ftj-decision-tree%2F</url>
    <content type="text"><![CDATA[本章主要讨论分类的决策树，它可以认为时if-then的集合。 决策树模型与学习 树的节点分为内部节点和叶节点，内部节点表示一个特征或属性，叶节点表示类别。 特征选择 信息增益 随机变量X的熵定义如下，熵越大，随机变量的不确定性就越大。 H(X)=−∑i=1npilogpiH(X)=-\sum_{i=1}^n p_ilogp_i H(X)=−​i=1​∑​n​​p​i​​logp​i​​ 条件熵： H(Y∣X)=∑i=1nP(X=xi)H(Y∣X=xi)H(Y|X)=\sum_{i=1}^n P(X=x_i)H(Y|X=x_i) H(Y∣X)=​i=1​∑​n​​P(X=x​i​​)H(Y∣X=x​i​​) 特征A对训练数据集D的信息增益(也叫互信息)定义如下： g(D,A)=H(D)−H(D∣A)g(D,A)=H(D)-H(D|A) g(D,A)=H(D)−H(D∣A) 信息增益比： gR(D,A)=g(D,A)HA(D)g_R(D,A)=\frac{g(D,A)}{H_A(D)} g​R​​(D,A)=​H​A​​(D)​​g(D,A)​​ 信息增益算法 输入：训练数据集D和特征A 输出：特征A对数据集D的信息增益g(D,A)g(D,A)g(D,A) 计算数据集的经验熵H(D) H(D)=−∑k=1K∣Ck∣∣D∣log2∣Ck∣∣D∣H(D) = -\sum_{k=1}^K \frac{|C_k|}{|D|}log_2\frac{|C_k|}{|D|} H(D)=−​k=1​∑​K​​​∣D∣​​∣C​k​​∣​​log​2​​​∣D∣​​∣C​k​​∣​​ 计算特征A对数据集D的经验条件熵H(D|A) H(D∣A)=−∑i=1n∣Di∣∣D∣H(Di)=−∑i=1n∣Di∣∣D∣∑k=1K∣Dik∣∣Di∣log2∣Dik∣∣Di∣H(D|A)=-\sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) = -\sum_{i=1}^n \frac{|D_i|}{|D|}\sum_{k=1}^K \frac{|D_{ik}|}{|D_i|} log_2\frac{|D_{ik}|}{|D_i|} H(D∣A)=−​i=1​∑​n​​​∣D∣​​∣D​i​​∣​​H(D​i​​)=−​i=1​∑​n​​​∣D∣​​∣D​i​​∣​​​k=1​∑​K​​​∣D​i​​∣​​∣D​ik​​∣​​log​2​​​∣D​i​​∣​​∣D​ik​​∣​​ 计算信息增益 g(D,A)=H(D)−H(D∣A)g(D,A)=H(D)-H(D|A) g(D,A)=H(D)−H(D∣A) 设训练数据集D，∣D∣|D|∣D∣ 表示样本的个数 设有个K类 Ck,k=1,2,...,KC_k, k=1,2,...,KC​k​​,k=1,2,...,K，∣Ck∣|C_k|∣C​k​​∣为属于类CkC_kC​k​​的样本个数，∣D∣=∑k=1KCk|D|=\sum_{k=1}^K C_k∣D∣=∑​k=1​K​​C​k​​ 特征A有n个不同的取值 {a1,a2,...,an}\{a_1,a_2,...,a_n\}{a​1​​,a​2​​,...,a​n​​}，A的取值将D划分为n个子集 {D1,D2,...,Dn}\{D_1,D_2,...,D_n\}{D​1​​,D​2​​,...,D​n​​} 决策树生成 ID3 算法，使用信息增益选择特征 C4.5 ，使用信息增益比选择特征 决策树剪枝：递归的从叶节点向上回缩，计算回缩之前的损失函数值，若损失函数更小，则进行剪枝，直到不能继续 CART算法 cart假设决策树时使用二叉树。 cart 最小二乘回归树生成算法： todo cart 剪枝： todo]]></content>
      <tags>
        <tag>machine-learning</tag>
        <tag>decision-tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四、朴素贝叶斯--统计机器学习方法]]></title>
    <url>%2F2017%2F10%2F15%2Ftj-bayesian%2F</url>
    <content type="text"><![CDATA[推荐文章，文章公式来源自 Pattern Recognition and Machine Learning，不大好看： http://blog.csdn.net/daunxx/article/details/51725086 https://honey0920.github.io/2017/07/11/prml-06/ 朴素贝叶斯法与学习分类 基本方法 朴素贝叶斯通过训练数据集学习联合概率分布，具体如下。 先验概率分布： P(Y=ck),k=1,2,...,KP(Y=c_k), k=1,2,...,K P(Y=c​k​​),k=1,2,...,K 条件概率分布： P(X=x∣Y=ck)=P(X1=x1,...,Xn=xn∣Y=ck),k=1,2,...,KP(X=x | Y=c_k) = P(X^1=x^1,...,X^n=x^n|Y=c_k), k=1,2,...,K P(X=x∣Y=c​k​​)=P(X​1​​=x​1​​,...,X​n​​=x​n​​∣Y=c​k​​),k=1,2,...,K 对于$ P(X=x | Y=c_k) ，假设 x^j 可取值有 S^j 个， Y可取值有K个，那么参数个数为 K\prod_{j=1}^n S_j $。 朴素贝叶斯对条件概率做了条件独立性假设： P(X=x∣Y=ck)=P(X1=x1,...,Xn=xn∣Y=ck)=∏j=1nP(Xj=xj∣Y=ck)P(X=x | Y=c_k) = P(X^1=x^1,...,X^n=x^n|Y=c_k)=\prod_{j=1}^n P(X^j=x^j|Y=c_k) P(X=x∣Y=c​k​​)=P(X​1​​=x​1​​,...,X​n​​=x​n​​∣Y=c​k​​)=​j=1​∏​n​​P(X​j​​=x​j​​∣Y=c​k​​) 后验概率实际上就是条件概率 对于输入x，根据学习到的模型，计算后验概率分布$ P(Y=c_k|X=x) $，将后验概率最大的类别作为x的输出。 y=argmaxckP(Y=ck)∏jP(Xj=xj∣Y=ck)y = arg\max_{c_k} P(Y=c_k)\prod_j P(X^j=x^j|Y=c_k) y=arg​c​k​​​max​​P(Y=c​k​​)​j​∏​​P(X​j​​=x​j​​∣Y=c​k​​) 极大似然估计 先验概率分布： P(Y=ck)=∑i=1NI(yi=ck)N,k=1,2,...,KP(Y=c_k)=\frac{\sum_{i=1}^N I(y_i=c_k)}{N}, k=1,2,...,K P(Y=c​k​​)=​N​​∑​i=1​N​​I(y​i​​=c​k​​)​​,k=1,2,...,K 条件概率分布： 设第j个特性$ x^j 可能取值的集合为 a_{j1}, a_{j2},…,a_{jS_j} $ P(Xj=ajl∣Y=ck)=∑i=1NI(xij=ajl,yi=ck)∑i=1NI(yi=ck),k=1,2,...,KP(X^j=a_{jl} | Y=c_k) = \frac{\sum_{i=1}^N I(x_i^j=a_{jl},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}, k=1,2,...,K P(X​j​​=a​jl​​∣Y=c​k​​)=​∑​i=1​N​​I(y​i​​=c​k​​)​​∑​i=1​N​​I(x​i​j​​=a​jl​​,y​i​​=c​k​​)​​,k=1,2,...,K 贝叶斯估计 条件概率的贝叶斯估计是： P(Xj=ajl∣Y=ck)=∑i=1NI(xij=ajl,yi=ck)+λ∑i=1NI(yi=ck)+Sjλ,k=1,2,...,KP(X^j=a_{jl} | Y=c_k) = \frac{\sum_{i=1}^N I(x_i^j=a_{jl},y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}, k=1,2,...,K P(X​j​​=a​jl​​∣Y=c​k​​)=​∑​i=1​N​​I(y​i​​=c​k​​)+S​j​​λ​​∑​i=1​N​​I(x​i​j​​=a​jl​​,y​i​​=c​k​​)+λ​​,k=1,2,...,K 当$ \lambda =0$ 时，就是极大似然估计，常$ \lambda =1$ ，这时称为拉普拉斯平滑。防止概率值为0的情况。 贝叶斯回归与分类 todo]]></content>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三、K邻近法--统计机器学习方法]]></title>
    <url>%2F2017%2F10%2F14%2Ftj-kd-tree%2F</url>
    <content type="text"><![CDATA[这里只讨论分类问题中的K邻近法 K邻近算法 输入：训练数据集 $ T={ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) } ，其中 x_i\in R^n $ 为实例的特征向量 ，$ y_i\in Y= {c_1,C_1,…,c_K} $ 为实例的类别， $ i=1,2,…,N ，实例特征向量 x 输出：实例 x 所属的类别 y $ 根据给定的距离度量，在训练集T中找出与x最邻近的k个点，涵盖这k个点的x的领域记作$ N_k(x) $ 在$ N_k(x) $中，根据分类决策规则（如多数表决）决定x的类别y y=argmaxcj∑xi∈Nk(x)I(yi=ci)y= arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i=c_i) y=arg​c​j​​​max​​​x​i​​∈N​k​​(x)​∑​​I(y​i​​=c​i​​) 其中I为指示函数，当 $ y_i=c_i $ 时，I为1，否则为0 距离度量 Lp距离，或者闵可夫斯基距离（Minkowski distance）， Lp(xi,xj)=(∑l=1n∣xil−xjl∣p)1pL_p(x_i, x_j) = ( \sum_{l=1}^n |x_i^l - x_j^l|^p ) ^{\frac{1}{p}} L​p​​(x​i​​,x​j​​)=(​l=1​∑​n​​∣x​i​l​​−x​j​l​​∣​p​​)​​p​​1​​​​ 当p=2时，称为欧氏距离，即： L2(xi,xj)=(∑l=1n∣xil−xjl∣2)12L_2(x_i, x_j) = ( \sum_{l=1}^n |x_i^l - x_j^l|^2 ) ^{\frac{1}{2}} L​2​​(x​i​​,x​j​​)=(​l=1​∑​n​​∣x​i​l​​−x​j​l​​∣​2​​)​​2​​1​​​​ 当p=1时，为曼哈顿距离，即： L1(xi,xj)=∑l=1n∣xil−xjl∣L_1(x_i, x_j) = \sum_{l=1}^n |x_i^l - x_j^l| L​1​​(x​i​​,x​j​​)=​l=1​∑​n​​∣x​i​l​​−x​j​l​​∣ K值选择与分类决策规则 k 一般取一个比较小的值 k邻近往往使用多数表决规则 kd 树 构造kd树 输入：k维空间数据集 $ T={x_1,x_2,…,x_N } $ ，其中 xi={xi1,xi2,...,xik}x_i=\{x_i^1,x_i^2,...,x_i^k \}x​i​​={x​i​1​​,x​i​2​​,...,x​i​k​​} ， i=1,2,...,Ni=1,2,...,Ni=1,2,...,N 输出：kd树 选择$ x^1 $为坐标轴，坐标的中位数为切分。与坐标轴垂直切分，左节点小于切分点，右节点大于切分点 重复：对深度为j的节点，选择$ x^l $为坐标轴, $ l=j(mod k)+1 $为坐标轴，进行切分 直到两个子区域没有实例存在 距离分析 搜索kd树 输入：已构造的kd树；目标点x 输出：x的最邻近点 在kd树中找出包含x的叶节点：从根节点出发，递归的向下访问，当前维度小于切分点，左移，否则右移，直到叶节点。 以此叶节点为“当前最近点” 递归向上回退 会退点更近，则替换“当前最近点” 父节点另一区域是否与超球体相交（目标点为球心，目标点与“当前最邻近点”距离为半径） 相交：检索区域内节点是否更近 不相交，继续向上回退 回退到根节点时，“当前最近点”即为x的最邻近点 举例分析： 首先找到叶节点D，最为当前最近点 回退到B，B不比D更近 判断B左侧即F所在区域是否与SD圆相交，否 回退到C，C不比D更近 检查C右侧即E所在区域与SD圆相交，是 检索C右侧区域，E比D更近，E为当前最近点 判断C左侧即G所在区域与SD圆相交，否 回退到A，A不比E更近，E为x最近点 代码实现 这里截取部分代码，参考 Machine Learning From Scratch。 1234567891011121314151617181920212223import numpy as npfrom utils import euclidean_distanceclass KNN(object): def __init__(self, k=5): self.k = k def _vote(self, neighbors): counts = np.bincount(neighbors[:, 1].astype('int')) return counts.argmax() def predict(self, x_test, x_train, y_train): y_pred = np.empty(x_test.shape[0]) for i, test_sample in enumerate(x_test): neighbors = np.empty((x_train.shape[0], 2)) for j, observed_sample in enumerate(x_train): distance = euclidean_distance(test_sample, observed_sample) label = y_train[j] neighbors[j] = [distance, label] k_nearest_neighbors = neighbors[neighbors[:, 0].argsort()][:self.k] y_pred[i] = self._vote(k_nearest_neighbors) return y_pred 测试代码 123456789101112131415161718192021222324252627282930from sklearn import datasets, neighbors, linear_modelfrom sklearn.model_selection import train_test_splitfrom supervised_learning.k_nearest_neighbors import KNNfrom utils.data_operation import accuracy_scoredef run_test(X, y): X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) knn = neighbors.KNeighborsClassifier() logistic = linear_model.LogisticRegression() knn_custom = KNN() print('KNN score: %f' % knn.fit(X_train, y_train).score(X_test, y_test)) print('LogisticRegression score: %f' % logistic.fit(X_train, y_train).score(X_test, y_test)) y_pred = knn_custom.predict(X_test, X_train, y_train) print('KNN custom score: %f' % accuracy_score(y_test, y_pred))def digit_test(): digits = datasets.load_digits() X_digits = digits.data y_digits = digits.target print("data length:&#123;&#125;".format(len(y_digits))) run_test(X_digits, y_digits)if __name__ == '__main__': digit_test() 结果，自己写的方法结果与sklearn一致，只是速度比较慢。 1234data length:1797KNN score: 0.986111LogisticRegression score: 0.961111KNN custom score: 0.986111]]></content>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二、感知机--统计机器学习方法]]></title>
    <url>%2F2017%2F10%2F14%2Ftj-perceptron%2F</url>
    <content type="text"><![CDATA[感知机模型 由输入空间到输出空间的如下函数称为感知机： f(x)=sign(w⋅x+b)f(x)=sign(w \cdot x+b) f(x)=sign(w⋅x+b) $ w\in R^n $ 叫做权值（weight），$ b\in R 叫做偏置（bias）， w \cdot x $是向量的内积，sign是符号函数，如下所示： sign(x)={+1,x≥0−1,x&lt;0sign(x)=\begin{cases}+1, &amp; x\geq 0 \\ -1, &amp; x&lt;0 \end{cases} sign(x)={​+1,​−1,​​​x≥0​x&lt;0​​ 感知机激励函数(激活函数) 激活函数是用来加入非线性因素的，若没有激活函数，多层的网络模型可累加为一层网络模型。 阶跃函数 sign(x)={+1,x≥0−1,x&lt;0sign(x)=\begin{cases}+1, &amp; x\geq 0 \\ -1, &amp; x&lt;0 \end{cases} sign(x)={​+1,​−1,​​​x≥0​x&lt;0​​ sigmoid函数 S(x)=11+e−xS(x)=\frac{1}{1+e^{-x}} S(x)=​1+e​−x​​​​1​​ S′(x)=S(x)∗(1−S(x))S&#x27;(x)= S(x)*(1-S(x)) S​′​​(x)=S(x)∗(1−S(x)) 感知机的学习策略 定义训练集： T={(x1,y1),(x2,y2),...,(xN,yN)}T=\{ (x_1,y_1),(x_2,y_2),...,(x_N,y_N) \} T={(x​1​​,y​1​​),(x​2​​,y​2​​),...,(x​N​​,y​N​​)} 其中$ x_i\in X=R^n $ ，$ y_i\in Y={+1, -1} $， $ i=1,2,…,N $，M为误分类点集合。感知机学习的损失函数定义为： L(W,b)=∑xi∈Myi(w⋅xi+b)L(W,b) = \sum_{x_i\in M} y_i(w \cdot x_i+b) L(W,b)=​x​i​​∈M​∑​​y​i​​(w⋅x​i​​+b) 感知机学习算法 感知机学习算法的原始形式 输入：训练数据集 $ T={ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) } ，其中 x_i\in R^n $ ，$ y_i\in {+1, -1} $， $ i=1,2,…,N ，学习率 \eta(0 \leq \eta \leq1) $ 输出：$ w,b $； 感知机模型 $ f(x)=sign(w \cdot x+b) $ 选取初始值$ w_0,b_0 $ 在训练集中选取数据$ x_i,y_i $ 如果$ y_i(w \cdot x_i+b) \leq 0 $ w←w+ηyixi w \leftarrow w+\eta y_ix_i w←w+ηy​i​​x​i​​ b←b+ηyi b \leftarrow b+\eta y_i b←b+ηy​i​​ 转至2，直至训练集中没有误分类点 感知机学习算法的对偶形式 输入：训练数据集 $ T={ (x_1,y_1),(x_2,y_2),…,(x_N,y_N) } ，其中 x_i\in R^n $ ，$ y_i\in {+1, -1} $， $ i=1,2,…,N ，学习率 \eta(0 \leq \eta \leq1) $ 输出：$ \alpha,b $； 感知机模型 $ f(x)=sign(\sum_{j=1}^N \alpha_j y_j x_j \cdot x+b) ； 其中 \alpha=(\alpha_1,\alpha_2,…,\alpha_N )^T $，每一位为常数 选取初始值$ \alpha \leftarrow 0, b \leftarrow 0 $ 在训练集中选取数据$ x_i,y_i $ 如果$ y_i(\sum_{j=1}^N \alpha_j y_j x_j \cdot x_i+b) \leq 0 $ αi←αi+η \alpha_i \leftarrow \alpha_i+\eta α​i​​←α​i​​+η b←b+ηyi b \leftarrow b+\eta y_i b←b+ηy​i​​ 转至2，直至训练集中没有误分类点]]></content>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一、统计机器学习概论--统计机器学习方法]]></title>
    <url>%2F2017%2F10%2F12%2Ftj-introduction%2F</url>
    <content type="text"><![CDATA[基本概念 监督学习：学习一个模型，使模型能对任意给定的输入，对其相应的输出做出一个很好的预测。 输入空间，输出空间 实例，特征向量，特征空间 回归问题：输入与输出变量均连续 分类问题：输出变量为有限的离散变量 标注问题：输入与输出均为变量序列 模型 模型就是所要学习的条件概率或决策函数 F={f∣Y=f(x)}F = \{ f | Y = f(x)\} F={f∣Y=f(x)} F={P∣P(Y∣X)}F=\{P|P(Y|X)\} F={P∣P(Y∣X)} 策略 常用损失函数： 0-1损失函数 L(Y,f(x))={1,Y≠f(x)0,Y=f(x)L(Y,f(x))=\begin{cases}1, &amp; Y \neq f(x)\\0, &amp; Y = f(x)\end{cases} L(Y,f(x))={​1,​0,​​​Y≠f(x)​Y=f(x)​​ 绝对损失函数 L(Y,F(x))=∣F−f(x)∣L(Y,F(x))=|F-f(x)| L(Y,F(x))=∣F−f(x)∣ 平方损失函数 L(Y,F(x))=(Y−f(x))2L(Y,F(x))=(Y-f(x))^2 L(Y,F(x))=(Y−f(x))​2​​ 对数损失函数 L(Y,F(x))=−logP(Y∣x)L(Y,F(x))=-logP(Y|x) L(Y,F(x))=−logP(Y∣x) 期望风险与经验风险 Rexp(f)≈Remp(f)=1N∑i=1NL(yi,f(xi))R_{exp}(f) \approx R_{emp}(f)=\frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i)) R​exp​​(f)≈R​emp​​(f)=​N​​1​​​i=1​∑​N​​L(y​i​​,f(x​i​​)) 算法 统计问题归结为最优化问题，统计学习的算法为求解最优化问题， 过拟合 一味提高对训练数据的预测能力，所选模型的复杂度往往比真实模型高，这种现象称为过拟合。 正则化 在经验风险上加上一个正则化项或者罚项 minf∈F1N∑i=1NL(yi,f(xi))+λJ(f)\min_{f \in F} \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda J(f) ​f∈F​min​​​N​​1​​​i=1​∑​N​​L(y​i​​,f(x​i​​))+λJ(f) 如在回归问题中，取w的二范式或者一范式 minf∈F1N∑i=1NL(yi,f(xi))+λ2∣w∣2\min_{f \in F} \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\frac{\lambda}{2} |w|^2 ​f∈F​min​​​N​​1​​​i=1​∑​N​​L(y​i​​,f(x​i​​))+​2​​λ​​∣w∣​2​​ minf∈F1N∑i=1NL(yi,f(xi))+λ∣w∣1\min_{f \in F} \frac{1}{N}\sum_{i=1}^N L(y_i,f(x_i))+\lambda |w|_1 ​f∈F​min​​​N​​1​​​i=1​∑​N​​L(y​i​​,f(x​i​​))+λ∣w∣​1​​ 交叉验证 随机的将样本分为训练集（training set），验证集（validation set）和测试集（test set）。 为解决样本不足情况： 简单交叉验证：训练集/测试集（7：3），选出测试误差最小的模型 N折交叉验证（N-fold cross valication）：将数据且分为N个不相交的子集，使用S-1个子集进行训练。 泛化能力 学习方法的泛化能力是指学习到的模型对位置数据的预测能力 生成式模型与判别式模型 生成式模型：由数据学习联合概率分布，然后求出条件概率密度作为预测模型。典型的有：朴素贝叶斯与隐马尔科夫模型 P(Y∣X)=P(X,Y)P(X)P(Y|X) = \frac{P(X,Y)}{P(X)} P(Y∣X)=​P(X)​​P(X,Y)​​ 判别式方法：由数据直接学习决策函数f(X)或者条件概率密度P(Y|X)作为预测函数。典型的有：logsitic回归/K邻近/感知机/决策树/最大熵/支持向量机/boosting/条件随机场 分类问题 精确率（precision）: P=TPTP+FPP = \frac{TP}{TP+FP} P=​TP+FP​​TP​​ 召回率（recall）: R=TPTP+FNR = \frac{TP}{TP+FN} R=​TP+FN​​TP​​ F1值： 2F1=1P+1R\frac{2}{F_1} = \frac{1}{P} +\frac{1}{R} ​F​1​​​​2​​=​P​​1​​+​R​​1​​ F1=2TP2TP+FP+FN{F_1} = \frac{2TP}{2TP+FP+FN} F​1​​=​2TP+FP+FN​​2TP​​ 另一个直观的图：]]></content>
      <tags>
        <tag>machine-learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[boston_housing]]></title>
    <url>%2F2017%2F10%2F09%2Fboston-housing%2F</url>
    <content type="text"><![CDATA[数据集介绍与简单操作 数据集介绍 数据下载地址： https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data 共有506条样本，样本无序，共14列数据，如下，主要用于回归测试。 CRIM - 犯罪率（per capita crime rate by town） ZN - 住宅地占比（proportion of residential land zoned for lots over 25,000 sq.ft.） INDUS - 商业区占地比（proportion of non-retail business acres per town.） CHAS - 是否湖景房（Charles River dummy variable (1 if tract bounds river; 0 otherwise)） NOX - PM2.5值 (nitric oxides concentration (parts per 10 million)) RM - 公寓平均房间数（average number of rooms per dwelling） AGE - 老住宅比例（proportion of owner-occupied units built prior to 1940） DIS - 距离城区距离（weighted distances to five Boston employment centres） RAD - 距离高速距离（index of accessibility to radial highways） TAX - 个税税率（full-value property-tax rate per $10,000） PTRATIO - 学生教师比（pupil-teacher ratio by town） B - 黑人比（ 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town） LSTAT - 低收入人群比（ % lower status of the population） MEDV - 自住房均价（Median value of owner-occupied homes in $1000’s） 简单操作 12345678910111213141516171819202122from sklearn.datasets import load_bostonboston = load_boston()print(boston.keys())# 数据维度print(boston.data.shape)# 特征名称print(boston.feature_names)# 数据集描述print(boston.DESCR)# 目标维度print(boston.target.shape)bos = pd.DataFrame(boston.data)print(bos.head())bos.columns = boston.feature_namesbos['PRICE'] = boston.target# 前5行样本数据print(bos.head())# 数据集均值方差等统计print(bos.describe()) 输出如下： 线性回归基础 12345678910111213141516171819202122from sklearn.datasets import load_bostonfrom sklearn.linear_model import LinearRegressionX_train, X_test, Y_train, Y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.33, random_state=5)print(X_train.shape)print(X_test.shape)print(Y_train.shape)print(Y_test.shape)lm = LinearRegression()lm.fit(X_train, Y_train)Y_pred = lm.predict(X_test)plt.scatter(Y_test, Y_pred)plt.xlabel(&quot;Prices: $Y_i$&quot;)plt.ylabel(&quot;Predicted prices: $\hat&#123;Y&#125;_i$&quot;)plt.title(&quot;Prices vs Predicted prices: $Y_i$ vs $\hat&#123;Y&#125;_i$&quot;)plt.show()mse = sklearn.metrics.mean_squared_error(Y_test, Y_pred)print(mse) 程序输出如下： 参考： Linear Regression on Boston Housing 预测波士顿房价–机器学习工程师纳米学位]]></content>
  </entry>
  <entry>
    <title><![CDATA[zookeeper-introduction]]></title>
    <url>%2F2017%2F09%2F30%2Fzookeeper-introduction%2F</url>
    <content type="text"><![CDATA[参考 http://yuzhouwan.com/posts/31915/]]></content>
  </entry>
  <entry>
    <title><![CDATA[flume 基础入门]]></title>
    <url>%2F2017%2F09%2F30%2Fflume-introduction%2F</url>
    <content type="text"><![CDATA[Flume简介 Apache Flume是一个分布式的、可靠的、可用的，从多种不同的源收集、聚集、移动大量日志数据到集中数据存储的系统。目前，只能运行在Unix服务器上。 Flume基于流式数据的、使用简单的（借助配置文件即可）、健壮的、容错的。 Flume的简单体现在：写一个source、channel、sink之后，一条命令就能操作成功。 Flume、Kafka实时进行数据收集，Storm、Spark实时数据处理，Impala实时查询。 数据流框架 基本概念： Event：一个数据单元，带有一个可选的消息头。 Flow：Event从源点到达目的点的迁移的抽象。 Client：操作位于源点处的Event，将其发送到Flume Agent。 Agent：一个独立的Flume进程，包含组件Source、Channel、Sink。 Source：用来消费传递到该组件的Event ，存入channel中。 Channel：中转Event的一个临时存储，保存有Source组件传递过来的Event。 Sink：从Channel中读取并移除Event，将Event传递到Flow Pipeline中的下一个Agent（如果有的话）。 核心组件 Source ExecSource: 以运行 Linux 命令的方式，持续的输出最新的数据，如 tail -F 文件名 指令，在这种方式下，取的文件名必须是指定的。 ExecSource 可以实现对日志的实时收集，但是存在Flume不运行或者指令执行出错时，将无法收集到日志数据，无法保证日志数据的完整性。 SpoolSource: 监测配置的目录下新增的文件，并将文件中的数据读取出来。需要注意两点：拷贝到 spool 目录下的文件不可以再打开编辑；spool 目录下不可包含相应的子目录。SpoolSource无法实现实时的收集数据，但可以设置以分钟的方式分割文件，趋于实时。 Channel Memory Channel, JDBC Channel , File Channel，Psuedo Transaction Channel。比较常见的是前三种 channel。 Sink Flume安装 与使用 安装 1234567wget http://mirrors.tuna.tsinghua.edu.cn/apache/flume/1.7.0/apache-flume-1.7.0-bin.tar.gz apache-flume-1.7.0-bin.tar.gztar -zxf flume-ng-1.5.0-cdh5.3.6.tar.gz# 配置Java环境，cat &quot;export JAVA_HOME=/XXX&quot; &gt;&gt; conf/flume-env.sh# 运行bin/flume-ng agent -n $agent_name -c conf -f conf/flume-conf.properties.template 参数说明： （1）agent参数表示，启动一个agent。 （2）-n或者–name 指定agent的名字，在配置文件中定义的agent的名称，例如下边的样例的名字是a1。 （3）-c或者–conf 指定配置文件所在的目录。该目录下要包含flume-env.sh文件。 （4）-f或者–conf-file 指定具体的配置文件。 -f指定的是一个Flume Agent的配置，存储在本地配置文件，该配置文件包含对source、channel、sink的属性配置，和其相关联形成数据流的配置。 Flume官方实例 配置文件 下面配置文件定义了一个agent，名为“a1”。a1有一个source，监听端口44444的数据。source、channel、sink的名称分别是r1、c1、k1。a1.channels.c1.type = memory 定义使用内存作为channel。 1234567891011121314151617181920212223# example.conf: A single-node Flume configuration#Name the components of the agent:a1.sources=r1a1.channels=c1a1.sinks=k1#Describe/Define the source:a1.sources.r1.type=netcata1.sources.r1.bind=hadoop-senior01.pmpa.coma1.sources.r1.port=44444#Describe/Define the channel:a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100# Describe the sinka1.sinks.k1.type = logger# Bind the source and sink to the channela1.sources.r1.channels = c1a1.sinks.k1.channel = c1 启动agent 1bin/flume-ng agent -n a1 -c conf -f conf/test-conf.properties -Dflume.root.logger=INFO,console todo! 参考 奉先 Flume介绍 Flume介绍]]></content>
  </entry>
  <entry>
    <title><![CDATA[hexo基本配置]]></title>
    <url>%2F2017%2F09%2F29%2Fhexo%E5%9F%BA%E6%9C%AC%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[官方网站提供了很好的文档，请首先查看：https://hexo.io/zh-cn/docs/index.html 初始化 123456npm install hexo-cli -g# npm install hexo --no-optional -ghexo init Blogcd Blognpm installhexo server 皮肤 github站点： https://github.com/iissnan/hexo-theme-next 相关文档：http://theme-next.iissnan.com/getting-started.html 1234567mkdir themes/nextcurl -L https://api.github.com/repos/iissnan/hexo-theme-next/tarball | tar -zxv -C themes/next --strip-components=1git tag -lgit checkout tags/v5.1.0# 修改_config.ymltheme: next 基本配置 12# 简体中文language: zh-Hans 写日志 基本操作 12hexo new [layout] &lt;title&gt; # layout: post/page/drafthexo publish draft &lt;title&gt; 添加图片 12post_asset_folder:truenpm install https://github.com/CodeFalling/hexo-asset-image --save 部署到github 1234567891011# 添加git依赖npm install hexo-deployer-git --save# 修改_config.ymldeploy: type: git repo: git@github.com:jiji262/jiji262.github.io.git branch: master# 部署hexo d/deploy 添加搜索 解决bug: https://github.com/iissnan/theme-next-docs/issues/162 安装步骤： http://theme-next.iissnan.com/third-party-services.html#algolia-search 常用命令 1234hexo algoliahexo shexo ghexo d]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
